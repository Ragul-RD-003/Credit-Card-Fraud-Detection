import os
import openai
from llm_guard import scan_prompt, scan_response

# Set your OpenAI API key
openai.api_key = os.getenv("OPENAI_API_KEY")

# In-memory user credit store (for demonstration purposes)
user_credits = {
    "user1": 100,  # User has 100 credits
    # Add more users as needed
}

def check_credits(user_id, cost):
    """Check if the user has enough credits."""
    return user_credits.get(user_id, 0) >= cost

def deduct_credits(user_id, cost):
    """Deduct credits from the user's balance."""
    if check_credits(user_id, cost):
        user_credits[user_id] -= cost
        return True
    return False

def process_request(user_id, prompt):
    """Process the user's prompt through OpenAI API with safety checks."""
    cost_per_request = 1  # Define cost per request

    # Check if user has enough credits
    if not check_credits(user_id, cost_per_request):
        return "Insufficient credits."

    # Scan the prompt for safety
    if not scan_prompt(prompt):
        return "Prompt failed safety checks."

    try:
        # Call OpenAI API
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
        reply = response['choices'][0]['message']['content']

        # Scan the response for safety
        if not scan_response(reply):
            return "Response failed safety checks."

        # Deduct credits after successful processing
        deduct_credits(user_id, cost_per_request)

        return reply

    except Exception as e:
        return f"An error occurred: {str(e)}"

# Example usage
if __name__ == "__main__":
    user_id = "user1"
    user_prompt = "Tell me an interesting fact about space."

    result = process_request(user_id, user_prompt)
    print(f"AI Response: {result}")
    print(f"Remaining Credits for {user_id}: {user_credits[user_id]}")
